AI-Generated Voice Detection (Multi-Language)

DhvaniSense is an API-based system that detects whether a given voice sample is AI-generated or Human, across multiple languages, using audio signal processing and a lightweight machine learning model enhanced with a custom Micro-Prosody Entropy Score (MPES).



ğŸ“‚ Project Structure
DhvaniSense/

â”œâ”€â”€ api/

â”‚   â”œâ”€â”€ app.py
                            # FastAPI application
â”‚   â”œâ”€â”€ model.py  
                            # Neural network model definition
â”‚   â”œâ”€â”€ audio_utils.py      
                            # Audio loading & feature extraction
â”‚   â””â”€â”€ mpes.py             # Micro-Prosody Entropy Score logic
â”‚
â”œâ”€â”€ training/

â”‚   â”œâ”€â”€ train.py          
                            # Model training script
â”‚   â””â”€â”€ dataset/

â”‚       â”œâ”€â”€ human/      
                            # Human voice MP3 files
â”‚       â””â”€â”€ ai/             
                            # AI-generated voice MP3 files
â”‚

â”œâ”€â”€ requirements.txt


â””â”€â”€ .gitignore




âš™ï¸ Prerequisites
1ï¸âƒ£ Python

Python 3.9 or above

2ï¸âƒ£ FFmpeg (Required)

FFmpeg is required for decoding MP3 audio files.

Download from: https://www.gyan.dev/ffmpeg/builds/

Add the bin/ directory to the system PATH

Verify installation:

ffmpeg -version




ğŸ“¦ Install Dependencies

From the project root directory:

pip install -r requirements.txt


ğŸ§ Preparing the Training Dataset
Dataset Format

Training data must be organized as follows:

training/dataset/
â”œâ”€â”€ human/
â”‚   â”œâ”€â”€ human1.mp3
â”‚   â”œâ”€â”€ human2.mp3
â”‚   â””â”€â”€ ...
â””â”€â”€ ai/
    â”œâ”€â”€ ai1.mp3
    â”œâ”€â”€ ai2.mp3
    â””â”€â”€ ...

Dataset Guidelines

Audio format: MP3

One speaker per file

Avoid long silences or heavy background noise

Balanced human and AI samples are recommended



ğŸ§  Training the Model

Run the training script from the project root:

cd training
python train.py

Training Pipeline

MP3 audio is resampled to 16 kHz

MFCC features are extracted

Labels:

0 â†’ Human

1 â†’ AI-generated

Model is trained using supervised learning

Trained weights are saved to:

api/model_weights.pth




ğŸ” Improving the Model

The model can be improved by:

Adding more MP3 samples to human/ and ai/

Adjusting training parameters in training/train.py

Number of epochs

Learning rate

Network size

Modifying the MPES logic in api/mpes.py to experiment with:

Pitch entropy thresholds

Additional micro-prosodic features

Retraining is required after any dataset or model change.




ğŸš€ Running the API

Start the API server from the project root:

cd api
uvicorn app:app --reload


The API will be available at:

http://127.0.0.1:8000


Interactive API documentation (Swagger UI):

http://127.0.0.1:8000/docs





ğŸ” API Authentication

All API requests must include the following header:

x-api-key: sk_dhvanisense_2026


Requests without a valid API key will be rejected.




ğŸ“¡ API Endpoint
POST /api/voice-detection
Request Body
{
  "language": "English",
  "audioFormat": "mp3",
  "audioBase64": "<BASE64_ENCODED_MP3>"
}

Response Example
{
  "status": "success",
  "language": "English",
  "classification": "AI_GENERATED",
  "confidenceScore": 0.63,
  "explanation": "Unnatural pitch consistency and robotic speech patterns detected"
}



ğŸ§ª Notes

Do not commit:

venv/

model_weights.pth

FFmpeg binaries

Keep the API request/response format unchanged

The system is designed to be language-agnostic across supported languages





ğŸ§  Core Approach

The system combines:

MFCC-based neural network classification

Micro-Prosody Entropy Score (MPES) to detect unnaturally smooth pitch behavior commonly observed in AI-generated speech.
